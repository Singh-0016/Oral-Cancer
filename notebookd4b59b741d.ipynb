{"cells":[{"cell_type":"markdown","metadata":{"id":"ET_fm_GMQmzv","papermill":{"duration":0.011766,"end_time":"2024-01-04T17:20:33.332921","exception":false,"start_time":"2024-01-04T17:20:33.321155","status":"completed"},"tags":[]},"source":["## Importing necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:56:21.730799Z","iopub.status.busy":"2024-01-24T17:56:21.730489Z","iopub.status.idle":"2024-01-24T17:56:35.847068Z","shell.execute_reply":"2024-01-24T17:56:35.845831Z","shell.execute_reply.started":"2024-01-24T17:56:21.730772Z"},"id":"Zq9xZkZbQ7Oi","outputId":"f7f0fef7-eeaa-4b22-cdd7-4f7f05648440","papermill":{"duration":12.99566,"end_time":"2024-01-04T17:20:46.339974","exception":false,"start_time":"2024-01-04T17:20:33.344314","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["pip install -U tensorflow-addons"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-24T17:56:35.850205Z","iopub.status.busy":"2024-01-24T17:56:35.849769Z","iopub.status.idle":"2024-01-24T17:57:03.551851Z","shell.execute_reply":"2024-01-24T17:57:03.550702Z","shell.execute_reply.started":"2024-01-24T17:56:35.850162Z"},"id":"88uNoI8kQmzx","outputId":"7f0362e6-3753-4474-a2d6-199d460d9813","papermill":{"duration":27.782743,"end_time":"2024-01-04T17:21:14.134436","exception":false,"start_time":"2024-01-04T17:20:46.351693","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip install visualkeras\n","import os\n","import warnings\n","import itertools\n","import cv2\n","import seaborn as sns\n","import pandas as pd\n","import numpy  as np\n","from PIL import Image\n","from sklearn.utils import class_weight\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import Counter\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import visualkeras\n","import plotly.express as px\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import multilabel_confusion_matrix\n","\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras import layers\n","from tensorflow.keras import regularizers\n","from sklearn.model_selection   import train_test_split\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","warnings.filterwarnings('ignore')\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"-oF47aPEQmzy","papermill":{"duration":0.012181,"end_time":"2024-01-04T17:21:14.163162","exception":false,"start_time":"2024-01-04T17:21:14.150981","status":"completed"},"tags":[]},"source":["## Setting up general parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:03.554105Z","iopub.status.busy":"2024-01-24T17:57:03.553354Z","iopub.status.idle":"2024-01-24T17:57:03.559452Z","shell.execute_reply":"2024-01-24T17:57:03.558462Z","shell.execute_reply.started":"2024-01-24T17:57:03.554063Z"},"id":"CiKJV6InQmzz","papermill":{"duration":0.022473,"end_time":"2024-01-04T17:21:14.198406","exception":false,"start_time":"2024-01-04T17:21:14.175933","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# General parameters\n","epochs = 20\n","image_size = 240\n","np.random.seed(42)\n","tf.random.set_seed(42)"]},{"cell_type":"markdown","metadata":{"id":"Y5ITZEe8Qmz1","papermill":{"duration":0.012166,"end_time":"2024-01-04T17:21:14.223228","exception":false,"start_time":"2024-01-04T17:21:14.211062","status":"completed"},"tags":[]},"source":["## Data Loading, Preperation and Visualization"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:03.584598Z","iopub.status.busy":"2024-01-24T17:57:03.584229Z","iopub.status.idle":"2024-01-24T17:57:03.589356Z","shell.execute_reply":"2024-01-24T17:57:03.5883Z","shell.execute_reply.started":"2024-01-24T17:57:03.584563Z"},"id":"lXA29ixCZibN","papermill":{"duration":0.019061,"end_time":"2024-01-04T17:21:14.295588","exception":false,"start_time":"2024-01-04T17:21:14.276527","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["folder_path = (\"OralCancer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:03.590862Z","iopub.status.busy":"2024-01-24T17:57:03.590579Z","iopub.status.idle":"2024-01-24T17:57:06.898341Z","shell.execute_reply":"2024-01-24T17:57:06.897367Z","shell.execute_reply.started":"2024-01-24T17:57:03.590838Z"},"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","\n","def load_and_process_dataset(folder_path):\n","    \"\"\"Loads and processes images using alternative while loop structures.\"\"\"\n","\n","    dataset = []\n","    labels = []\n","    class_folders = ['non-cancer', 'cancer']\n","\n","    class_index = 0\n","    while class_index < len(class_folders):\n","        class_folder = class_folders[class_index]\n","        images_path = os.path.join(folder_path, class_folder)\n","\n","        image_index = 0\n","        while True:  # Loop infinitely until a \"break\" occurs\n","            try:\n","                image_name = os.listdir(images_path)[image_index]  # Access by index\n","                image_path = os.path.join(images_path, image_name)\n","\n","                image = cv2.imread(image_path)\n","                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n","                image = cv2.resize(image, (240, 240))\n","\n","                dataset.append(image)\n","                labels.append(class_index)  # Use class_index directly for labels\n","\n","                image_index += 1\n","            except IndexError:  # Handle end of image list\n","                break  # Exit the inner loop\n","\n","        class_index += 1\n","\n","    return np.array(dataset), np.array(labels)\n","\n","# Assuming `folder_path` is already a string\n","dataset, labels = load_and_process_dataset(folder_path)\n","\n","# Convert to NumPy arrays\n","dataset = np.array(dataset)\n","lab = np.array(labels)\n","\n","# Print shapes\n","print(dataset.shape, labels.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:06.899902Z","iopub.status.busy":"2024-01-24T17:57:06.899601Z","iopub.status.idle":"2024-01-24T17:57:06.914023Z","shell.execute_reply":"2024-01-24T17:57:06.912595Z","shell.execute_reply.started":"2024-01-24T17:57:06.899876Z"},"trusted":true},"outputs":[],"source":["# Convert to NumPy arrays and print shapes in a single line\n","print(np.array(dataset).shape, np.array(labels).shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:06.915537Z","iopub.status.busy":"2024-01-24T17:57:06.915218Z","iopub.status.idle":"2024-01-24T17:57:06.936284Z","shell.execute_reply":"2024-01-24T17:57:06.935295Z","shell.execute_reply.started":"2024-01-24T17:57:06.915513Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(np.array(dataset), np.array(labels), test_size=0.2, shuffle=True, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:06.937947Z","iopub.status.busy":"2024-01-24T17:57:06.937628Z","iopub.status.idle":"2024-01-24T17:57:06.94507Z","shell.execute_reply":"2024-01-24T17:57:06.944052Z","shell.execute_reply.started":"2024-01-24T17:57:06.937922Z"},"id":"x55eqTuvQmz7","papermill":{"duration":0.061865,"end_time":"2024-01-04T17:21:18.820927","exception":false,"start_time":"2024-01-04T17:21:18.759062","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_state(state):\n","    images = [load_img(os.path.join(folder_path, state, img_name), target_size=(image_size, image_size))\n","              for img_name in os.listdir(os.path.join(folder_path, state))[:9]]\n","\n","    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n","    for ax, img in zip(axes.flat, images):\n","        ax.imshow(img)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:06.949775Z","iopub.status.busy":"2024-01-24T17:57:06.94933Z","iopub.status.idle":"2024-01-24T17:57:08.942683Z","shell.execute_reply":"2024-01-24T17:57:08.941583Z","shell.execute_reply.started":"2024-01-24T17:57:06.949748Z"},"id":"gFl8NMpaQmz8","outputId":"27d40ee1-a0f5-4c92-f3b3-3befe9703590","papermill":{"duration":1.947212,"end_time":"2024-01-04T17:21:20.781322","exception":false,"start_time":"2024-01-04T17:21:18.83411","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["plot_state('cancer')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:08.944196Z","iopub.status.busy":"2024-01-24T17:57:08.9439Z","iopub.status.idle":"2024-01-24T17:57:10.638521Z","shell.execute_reply":"2024-01-24T17:57:10.637418Z","shell.execute_reply.started":"2024-01-24T17:57:08.944169Z"},"id":"NIyBJpAEQmz9","outputId":"b14f7cec-14cd-4e8a-95ac-f92155f847ab","papermill":{"duration":1.68248,"end_time":"2024-01-04T17:21:22.496103","exception":false,"start_time":"2024-01-04T17:21:20.813623","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["plot_state(\"non-cancer\")"]},{"cell_type":"markdown","metadata":{"id":"-npZ5GGLQmz_","papermill":{"duration":0.043161,"end_time":"2024-01-04T17:21:22.587318","exception":false,"start_time":"2024-01-04T17:21:22.544157","status":"completed"},"tags":[]},"source":["# ****** Baseline Model CNN**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:10.640068Z","iopub.status.busy":"2024-01-24T17:57:10.639763Z","iopub.status.idle":"2024-01-24T17:57:12.099869Z","shell.execute_reply":"2024-01-24T17:57:12.098894Z","shell.execute_reply.started":"2024-01-24T17:57:10.640042Z"},"id":"sh9MCg5LQm0A","papermill":{"duration":4.641081,"end_time":"2024-01-04T17:21:27.270498","exception":false,"start_time":"2024-01-04T17:21:22.629417","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.models import Model\n","\n","# Input layer\n","input_layer = Input(shape=(image_size, image_size, 3))\n","\n","# Convolutional layers\n","conv1 = Conv2D(filters=32, kernel_size=(3, 3), strides=(2, 2), activation=\"relu\", padding=\"valid\")(input_layer)\n","maxpool1 = MaxPooling2D((2, 2))(conv1)\n","\n","conv2 = Conv2D(filters=32, kernel_size=(3, 3), strides=(2, 2), activation=\"relu\", padding=\"valid\")(maxpool1)\n","maxpool2 = MaxPooling2D((2, 2))(conv2)\n","\n","# Flatten layer\n","flatten = Flatten()(maxpool2)\n","\n","# Dense layers with regularization\n","dense1 = Dense(units=64, activation='relu',\n","               kernel_regularizer=regularizers.L1L2(l1=1e-3, l2=1e-3),\n","               bias_regularizer=regularizers.L2(1e-2),\n","               activity_regularizer=regularizers.L2(1e-3))(flatten)\n","dropout = Dropout(0.5)(dense1)\n","\n","output_layer = Dense(units=1, activation='sigmoid')(dropout)\n","\n","# Define the model\n","model = Model(inputs=input_layer, outputs=output_layer)\n","\n","\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import BinaryCrossentropy\n","\n","# Compile the model\n","model.compile(\n","    optimizer=Adam(),\n","    loss=BinaryCrossentropy(),\n","    metrics=['accuracy']\n",")\n","\n","# Print the model summary\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:12.101457Z","iopub.status.busy":"2024-01-24T17:57:12.101137Z","iopub.status.idle":"2024-01-24T17:57:12.371375Z","shell.execute_reply":"2024-01-24T17:57:12.370457Z","shell.execute_reply.started":"2024-01-24T17:57:12.101428Z"},"id":"Bj7Tm6S8Qm0E","outputId":"05625bf2-60b1-4d51-c056-1d4a45e6c3ba","papermill":{"duration":0.308191,"end_time":"2024-01-04T17:21:27.794683","exception":false,"start_time":"2024-01-04T17:21:27.486492","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["plot_model(model, show_shapes=True, show_layer_names=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:12.372852Z","iopub.status.busy":"2024-01-24T17:57:12.372569Z","iopub.status.idle":"2024-01-24T17:57:12.479826Z","shell.execute_reply":"2024-01-24T17:57:12.478895Z","shell.execute_reply.started":"2024-01-24T17:57:12.372828Z"},"id":"6AJ6R0F3Qm0G","outputId":"2c88eab6-b50b-4ef3-8490-74c3f5b71327","papermill":{"duration":0.097622,"end_time":"2024-01-04T17:21:27.942868","exception":false,"start_time":"2024-01-04T17:21:27.845246","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["visualkeras.layered_view(model, legend=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:12.481305Z","iopub.status.busy":"2024-01-24T17:57:12.480987Z","iopub.status.idle":"2024-01-24T17:57:12.48855Z","shell.execute_reply":"2024-01-24T17:57:12.487668Z","shell.execute_reply.started":"2024-01-24T17:57:12.481278Z"},"trusted":true},"outputs":[],"source":["class_weights = {}\n","for cls in np.unique(y_train):\n","    count = np.sum(y_train == cls)\n","    class_weights[cls] = 1.0 / count\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:12.490223Z","iopub.status.busy":"2024-01-24T17:57:12.489866Z","iopub.status.idle":"2024-01-24T17:57:31.776137Z","shell.execute_reply":"2024-01-24T17:57:31.77536Z","shell.execute_reply.started":"2024-01-24T17:57:12.490188Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","# Assuming 'model', 'x_train', 'y_train', 'x_test', and 'y_test' are already defined\n","\n","# Calculate class weights\n","class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n","class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n","\n","# Compile the model\n","model.compile(\n","    optimizer=Adam(),\n","    loss=BinaryCrossentropy(),\n","    metrics=['accuracy']\n",")\n","\n","# Train the model\n","history = model.fit(\n","    x_train, y_train,\n","    epochs=200,\n","    class_weight=class_weights_dict,\n","    validation_data=(x_test, y_test),\n","    verbose=1\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"wzlKNxvPQm0K","papermill":{"duration":0.083906,"end_time":"2024-01-04T17:21:50.51916","exception":false,"start_time":"2024-01-04T17:21:50.435254","status":"completed"},"tags":[]},"source":["# ## Evaluating CNN Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:31.777672Z","iopub.status.busy":"2024-01-24T17:57:31.77737Z","iopub.status.idle":"2024-01-24T17:57:32.459742Z","shell.execute_reply":"2024-01-24T17:57:32.458808Z","shell.execute_reply.started":"2024-01-24T17:57:31.777648Z"},"id":"FWULyhZZQm0M","outputId":"da915c95-efd5-4e18-9dc6-98b54a4fac13","papermill":{"duration":0.62641,"end_time":"2024-01-04T17:21:51.229544","exception":false,"start_time":"2024-01-04T17:21:50.603134","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import seaborn as sns\n","\n","sns.set_style(\"whitegrid\")\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n","\n","ax1.plot(history.history['loss'], label='Training Loss')\n","ax1.plot(history.history['val_loss'], label='Validation Loss')\n","ax1.set_title('Loss', fontsize=14)\n","ax1.legend()\n","\n","ax2.plot(history.history['accuracy'], label='Training Accuracy')\n","ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","ax2.set_title('Accuracy', fontsize=14)\n","ax2.legend()\n","\n","fig.suptitle('Optimizer: Adam', fontsize=16)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:32.461327Z","iopub.status.busy":"2024-01-24T17:57:32.460978Z","iopub.status.idle":"2024-01-24T17:57:32.541944Z","shell.execute_reply":"2024-01-24T17:57:32.540969Z","shell.execute_reply.started":"2024-01-24T17:57:32.461296Z"},"trusted":true},"outputs":[],"source":["# Assuming 'model', 'x_test', and 'y_test' are already defined\n","\n","# Evaluate the model on the test set\n","test_loss, test_accuracy = model.evaluate(x_test, y_test)\n","\n","# Print the accuracy\n","print('The accuracy of the baseline model CNN is {:.2f}%!'.format(test_accuracy * 100))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:32.543548Z","iopub.status.busy":"2024-01-24T17:57:32.543232Z","iopub.status.idle":"2024-01-24T17:57:32.712206Z","shell.execute_reply":"2024-01-24T17:57:32.711388Z","shell.execute_reply.started":"2024-01-24T17:57:32.543521Z"},"trusted":true},"outputs":[],"source":["predictions = model.predict(x_test)\n","y_pred = (predictions >= 0.5).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:32.713657Z","iopub.status.busy":"2024-01-24T17:57:32.713336Z","iopub.status.idle":"2024-01-24T17:57:32.723384Z","shell.execute_reply":"2024-01-24T17:57:32.722583Z","shell.execute_reply.started":"2024-01-24T17:57:32.71363Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","def plot_confusion_matrix_enhanced(cm, classes, title='Confusion Matrix', cmap='viridis'):\n","    \"\"\"\n","    Plots a confusion matrix with a distinct style and colormap.\n","    \"\"\"\n","\n","    # Normalize for better visualization\n","    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    # Create a DataFrame for flexible plotting\n","    df_cm = pd.DataFrame(cm_norm, index=classes, columns=classes)\n","\n","    # Employ Seaborn for a visually appealing heatmap\n","    sns.heatmap(df_cm, annot=True, fmt='.2f', cmap=cmap, vmin=0, vmax=1)\n","\n","    # Customize labels and title\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(title)\n","\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:32.724926Z","iopub.status.busy":"2024-01-24T17:57:32.72464Z","iopub.status.idle":"2024-01-24T17:57:32.991394Z","shell.execute_reply":"2024-01-24T17:57:32.990556Z","shell.execute_reply.started":"2024-01-24T17:57:32.724901Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Compute confusion matrix\n","cnf_matrix = confusion_matrix(y_test, y_pred)\n","\n","# Normalize for better visualization\n","cnf_matrix_norm = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n","\n","# Employ a triangular structure for visual clarity\n","mask = np.triu(np.ones_like(cnf_matrix_norm, dtype=bool))\n","\n","# Create a visually distinct heatmap with a different colormap\n","sns.heatmap(cnf_matrix_norm, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', vmin=0, vmax=1,\n","            linewidths=0.5, cbar_kws={'label': 'Normalized Values'})\n","\n","# Customize labels and title\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.title('Confusion Matrix (Triangular Structure)')\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:32.992747Z","iopub.status.busy":"2024-01-24T17:57:32.992474Z","iopub.status.idle":"2024-01-24T17:57:33.141937Z","shell.execute_reply":"2024-01-24T17:57:33.141007Z","shell.execute_reply.started":"2024-01-24T17:57:32.992723Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","# Compute confusion matrix\n","cnf_matrix = confusion_matrix(y_test, y_pred)\n","\n","# Create a hexagonal mask for a visually striking shape\n","mask = np.triu(np.ones_like(cnf_matrix, dtype=bool)) & np.tril(np.ones_like(cnf_matrix, dtype=bool), k=-1)\n","\n","# Plot using Seaborn with distinct customizations\n","sns.heatmap(\n","    cnf_matrix,\n","    annot=True,\n","    fmt='.2f',\n","    cmap='cubehelix',  # Employ a vibrant colormap\n","    mask=mask,\n","    linewidths=0.5,\n","    cbar=False,\n","    xticklabels=[\"non-cancer\", \"cancer\"],\n","    yticklabels=[\"non-cancer\", \"cancer\"]\n",")\n","plt.title('Confusion Matrix - Hexagonal Emphasis')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"Ou2YZ5ZRQm0T","papermill":{"duration":0.082739,"end_time":"2024-01-04T17:21:52.653468","exception":false,"start_time":"2024-01-04T17:21:52.570729","status":"completed"},"tags":[]},"source":["# Let's build Vision Transformers(ViT) from the scratch"]},{"cell_type":"markdown","metadata":{},"source":["# Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:33.144269Z","iopub.status.busy":"2024-01-24T17:57:33.143563Z","iopub.status.idle":"2024-01-24T17:57:33.15061Z","shell.execute_reply":"2024-01-24T17:57:33.149559Z","shell.execute_reply.started":"2024-01-24T17:57:33.144229Z"},"id":"56IG2oPPQm0T","papermill":{"duration":0.093145,"end_time":"2024-01-04T17:21:52.82994","exception":false,"start_time":"2024-01-04T17:21:52.736795","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["image_size = 224\n","batch_size = 16\n","learning_rate = 0.001\n","weight_decay = 0.0001\n","num_epochs = 1\n","\n","patch_size = 7  # Size of the patches to be extract from the input images\n","num_patches = (image_size // patch_size) ** 2\n","projection_dim = 64\n","num_heads = 4\n","transformer_units = [\n","    projection_dim * 2,\n","    projection_dim,\n","]  # Size of the transformer layers\n","transformer_layers = 8\n","mlp_head_units = [56, 28]  # Size of the dense layers of the final classifier\n"]},{"cell_type":"markdown","metadata":{"id":"qcJk_y7TQm0U","papermill":{"duration":0.082644,"end_time":"2024-01-04T17:21:52.995428","exception":false,"start_time":"2024-01-04T17:21:52.912784","status":"completed"},"tags":[]},"source":["## Data augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:33.152429Z","iopub.status.busy":"2024-01-24T17:57:33.151915Z","iopub.status.idle":"2024-01-24T17:57:33.486241Z","shell.execute_reply":"2024-01-24T17:57:33.485222Z","shell.execute_reply.started":"2024-01-24T17:57:33.15238Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","# Create normalization layer with adaptable axes\n","normalization_layer = tf.keras.layers.Normalization(axis=[-1])  # Adapt across channels\n","\n","# Create augmentation pipeline using functional API\n","data_augmentation = tf.keras.Sequential(\n","    [\n","        normalization_layer,\n","        tf.keras.layers.Resizing(image_size, image_size),\n","        tf.keras.layers.RandomFlip(\"horizontal\"),\n","        tf.keras.layers.RandomRotation(factor=0.02),\n","        tf.keras.layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n","    ],\n","    name=\"data_augmentation\",\n",")\n","\n","# Explicitly extract normalization layer for adaptation\n","norm_layer = data_augmentation.get_layer(index=0)  # Access by index\n","norm_layer.adapt(x_train)  # Adapt to training data\n"]},{"cell_type":"markdown","metadata":{"id":"k3i1xfgwQm0W","papermill":{"duration":0.082597,"end_time":"2024-01-04T17:21:53.616647","exception":false,"start_time":"2024-01-04T17:21:53.53405","status":"completed"},"tags":[]},"source":["## Multi-layer perceptron"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:33.488558Z","iopub.status.busy":"2024-01-24T17:57:33.487752Z","iopub.status.idle":"2024-01-24T17:57:33.494589Z","shell.execute_reply":"2024-01-24T17:57:33.493595Z","shell.execute_reply.started":"2024-01-24T17:57:33.488519Z"},"id":"K-CE_HCuQm0X","papermill":{"duration":0.090776,"end_time":"2024-01-04T17:21:53.789495","exception":false,"start_time":"2024-01-04T17:21:53.698719","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","def mlp_enhanced(x, hidden_units, dropout_rate):\n","    \"\"\"Creates a multi-layer perceptron with functional layers and enhancements.\"\"\"\n","\n","    for units in hidden_units:\n","        x = tf.keras.layers.Dense(units)(x)  # Apply linear transformation first\n","        x = tf.keras.layers.BatchNormalization()(x)  # Normalize activations\n","        x = tf.nn.swish(x)  # Explore a different activation function\n","        x = tf.keras.layers.Dropout(dropout_rate)(x)\n","\n","    return x\n"]},{"cell_type":"markdown","metadata":{"id":"NQagAzB8Qm0Y","papermill":{"duration":0.08445,"end_time":"2024-01-04T17:21:53.957427","exception":false,"start_time":"2024-01-04T17:21:53.872977","status":"completed"},"tags":[]},"source":["## Implement patch creation as a layer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:33.496165Z","iopub.status.busy":"2024-01-24T17:57:33.49589Z","iopub.status.idle":"2024-01-24T17:57:33.510973Z","shell.execute_reply":"2024-01-24T17:57:33.510069Z","shell.execute_reply.started":"2024-01-24T17:57:33.496142Z"},"id":"0nK7FQ8lQm0Z","papermill":{"duration":0.094272,"end_time":"2024-01-04T17:21:54.138155","exception":false,"start_time":"2024-01-04T17:21:54.043883","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","def create_patches_enhanced(images, patch_size):\n","    \"\"\"Extracts patches using functional API and potential optimization.\"\"\"\n","\n","    patches = tf.keras.layers.Lambda(\n","        lambda x: tf.image.extract_patches(\n","            images=x,\n","            sizes=[1, patch_size, patch_size, 1],\n","            strides=[1, patch_size, patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","    )(images)\n","\n","    patch_dims = patches.shape[-1]\n","    patches = tf.keras.layers.Reshape([tf.shape(images)[0], -1, patch_dims])(patches)\n","\n","    # Potential optimization for specific hardware (explore if applicable)\n","    patches = tf.experimental.tensorrt.convert_to_tensorrt(patches)\n","\n","    return patches\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:57:33.512133Z","iopub.status.busy":"2024-01-24T17:57:33.511882Z","iopub.status.idle":"2024-01-24T17:59:09.942822Z","shell.execute_reply":"2024-01-24T17:59:09.941916Z","shell.execute_reply.started":"2024-01-24T17:57:33.512111Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from ipywidgets import interact, IntSlider  # Import the interact function\n","\n","def visualize_patches(image, image_size, patch_size):\n","    \"\"\"Visualizes patches using a clear functional approach and interactive controls.\"\"\"\n","\n","    # Resize and extract patches\n","    resized_image = tf.image.resize(image, (image_size, image_size))\n","    patches = tf.image.extract_patches(\n","        resized_image,\n","        sizes=[1, patch_size, patch_size, 1],\n","        strides=[1, patch_size, patch_size, 1],\n","        rates=[1, 1, 1, 1],\n","        padding=\"VALID\",\n","    )\n","    patches = tf.reshape(patches, [-1, patch_size, patch_size, 3])\n","\n","    # Interactive exploration of patches\n","    @interact(patch_index=IntSlider(value=0, min=0, max=patches.shape[0] - 1))\n","    def display_patch(patch_index):\n","        plt.figure(figsize=(4, 4))\n","        plt.imshow(patches[patch_index].numpy().astype(\"uint8\"))\n","        plt.axis(\"off\")\n","        plt.title(f\"Patch {patch_index + 1}\")\n","        plt.show()\n","\n","    # Print information concisely\n","    print(f\"Image size: {image_size} X {image_size}\")\n","    print(f\"Patch size: {patch_size} X {patch_size}\")\n","    print(f\"Patches per image: {patches.shape[0]}\")\n","    print(f\"Elements per patch: {patches.shape[-1] * patches.shape[-2]}\")\n","\n","    # Visualize all patches in a grid\n","    n = int(np.sqrt(patches.shape[0]))\n","    plt.figure(figsize=(10, 10))\n","    for i, patch in enumerate(patches):\n","        plt.subplot(n, n, i + 1)\n","        plt.imshow(patch.numpy().astype(\"uint8\"))\n","        plt.axis(\"off\")\n","    plt.show()\n","\n","# Example usage\n","image = x_train[np.random.choice(range(x_train.shape[0]))]\n","visualize_patches(tf.expand_dims(image, axis=0), image_size, patch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:59:09.947549Z","iopub.status.busy":"2024-01-24T17:59:09.947251Z","iopub.status.idle":"2024-01-24T17:59:11.872481Z","shell.execute_reply":"2024-01-24T17:59:11.87157Z","shell.execute_reply.started":"2024-01-24T17:59:09.947524Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","\n","# Assuming 'x_train' is already defined\n","\n","# Choose a random image from x_train\n","image = x_train[np.random.choice(range(x_train.shape[0]))]\n","\n","# Display the original image\n","plt.figure(figsize=(8, 8))\n","plt.imshow(image.astype(\"uint8\"))\n","plt.axis(\"off\")\n","plt.show()\n","\n","# Resize the image\n","image_size = 64\n","resized_image = tf.image.resize(tf.convert_to_tensor([image]), size=(image_size, image_size))\n","\n","# Extract patches\n","patch_size = 16\n","patches = tf.image.extract_patches(images=resized_image, sizes=[1, patch_size, patch_size, 1], strides=[1, patch_size, patch_size, 1], rates=[1, 1, 1, 1], padding='VALID')\n","\n","# Reshape patches\n","patches = tf.reshape(patches, [-1, patch_size, patch_size, 3])\n","\n","# Display patches\n","n = int(np.sqrt(patches.shape[0]))\n","plt.figure(figsize=(8, 8))\n","for i, patch in enumerate(patches):\n","    ax = plt.subplot(n, n, i + 1)\n","    plt.imshow(patch.numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")\n","\n","print(f\"Image size: {image_size} X {image_size}\")\n","print(f\"Patch size: {patch_size} X {patch_size}\")\n","print(f\"Patches per image: {patches.shape[0]}\")\n","print(f\"Elements per patch: {patches.shape[-1]}\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"SbJzzeB0Qm0c","papermill":{"duration":0.093548,"end_time":"2024-01-04T17:21:59.69582","exception":false,"start_time":"2024-01-04T17:21:59.602272","status":"completed"},"tags":[]},"source":["## Creating the patch encoder\n","The PatchEncoder layer will linearly transform a patch by projecting it into a vector of size projection_dim. In addition, it adds a learnable position embedding to the projected vector."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:59:11.87468Z","iopub.status.busy":"2024-01-24T17:59:11.873706Z","iopub.status.idle":"2024-01-24T17:59:11.881541Z","shell.execute_reply":"2024-01-24T17:59:11.880614Z","shell.execute_reply.started":"2024-01-24T17:59:11.874644Z"},"id":"S-CcvHs7Qm0e","papermill":{"duration":0.103045,"end_time":"2024-01-04T17:21:59.8935","exception":false,"start_time":"2024-01-04T17:21:59.790455","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","def patch_encoder_enhanced(patches, num_patches, projection_dim):\n","    \"\"\"Encodes patches using functional API, learned positional embeddings, and layer normalization.\"\"\"\n","\n","    projected = tf.keras.layers.Dense(projection_dim)(patches)\n","\n","    # Learned positional embeddings\n","    positions = tf.range(start=0, limit=num_patches, delta=1)\n","    positional_embeddings = tf.keras.layers.Embedding(\n","        input_dim=num_patches, output_dim=projection_dim\n","    )(positions)\n","\n","    encoded = projected + positional_embeddings\n","\n","    # Layer normalization for stability\n","    encoded = tf.keras.layers.LayerNormalization()(encoded)\n","\n","    return encoded\n"]},{"cell_type":"markdown","metadata":{"id":"zrM8zvGsQm0f","papermill":{"duration":0.093364,"end_time":"2024-01-04T17:22:00.080033","exception":false,"start_time":"2024-01-04T17:21:59.986669","status":"completed"},"tags":[]},"source":["## Building the ViT\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:59:11.882979Z","iopub.status.busy":"2024-01-24T17:59:11.882649Z","iopub.status.idle":"2024-01-24T17:59:24.315192Z","shell.execute_reply":"2024-01-24T17:59:24.313994Z","shell.execute_reply.started":"2024-01-24T17:59:11.882945Z"},"trusted":true},"outputs":[],"source":["pip install vit_keras"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-24T17:59:24.317034Z","iopub.status.busy":"2024-01-24T17:59:24.316698Z","iopub.status.idle":"2024-01-24T17:59:24.330755Z","shell.execute_reply":"2024-01-24T17:59:24.329765Z","shell.execute_reply.started":"2024-01-24T17:59:24.317004Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","# parameters for model\n","batch_size = 256\n","image_size = 224\n","\n","patch_size = 16\n","\n","projection_dim = 64\n","transformer_units = [projection_dim * 2, projection_dim, ]\n","transformer_layers = 8\n","mlp_head_units = [2048, 1024]\n","\n","\n","def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x\n","\n","\n","class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        patch_dims = patches.shape[-1]\n","        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n","        return patches\n","\n","\n","class PatchEncoder(layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super(PatchEncoder, self).__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","\n","    def call(self, patch):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patch) + self.position_embedding(positions)\n","        return encoded"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":908874,"sourceId":1541124,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"},"papermill":{"default_parameters":{},"duration":144.798167,"end_time":"2024-01-04T17:22:54.633479","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-04T17:20:29.835312","version":"2.4.0"}},"nbformat":4,"nbformat_minor":4}
